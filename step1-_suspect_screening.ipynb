{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# 1. Load Files Here: (extension name needed)\n",
    "input_folder = 'test/BP_Samples/corrected/neg/'\n",
    "csv_files = glob.glob(os.path.join(input_folder, '*.csv'))\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for input_file_path in csv_files:\n",
    "    if input_file_path.endswith('_processed.csv'):\n",
    "        continue\n",
    "    # Raw Data:\n",
    "    input_file = pd.read_csv(input_file_path, skiprows=6).dropna()\n",
    "    # Library File:\n",
    "    library_file = pd.read_csv('../PFAS_libraries/PFAS_lib_C2-30.csv')\n",
    "    # Output File:\n",
    "    # output_file_name = os.path.splitext(os.path.basename(input_file_path))[0] + '_processed.csv'\n",
    "    # output_file = os.path.join(input_folder, output_file_name)\n",
    "    # if os.path.exists(output_file):\n",
    "    #     continue\n",
    "\n",
    "    # 2. Set Parameters Here:\n",
    "    # S_to_N_threshold = 3\n",
    "    ppm_error = 0.2\n",
    "    PROTON = 1.00727647\n",
    "    \n",
    "    # 3. Input File Peaks List\n",
    "    ## 3a. Check input file\n",
    "    # input_file = input_file.loc[input_file['S/N'] >= S_to_N_threshold, :]\n",
    "    raw_pks = len(input_file.index)\n",
    "    \n",
    "    ## 3b. Convert all of the uppercase column labels to lowercase and replace the spaces with `_`.\n",
    "    input_file.columns = (\n",
    "        input_file.columns.str.lower()\n",
    "        .str.strip()\n",
    "        .str.replace(r'\\s+', '_', regex=True)\n",
    "    )\n",
    "    input_file['nominal_mass'] = round(input_file['peak_location']).astype(int)\n",
    "    \n",
    "    ## 3c. Calculate experimental KMD values of CF2, CH2, and C2H4O series\n",
    "    input_file['KMD_CF2'] = (input_file['peak_location'].round() - input_file['peak_location']*50/49.99681).round(4)\n",
    "    input_file['KMD_CH2'] = (input_file['peak_location'].round() - input_file['peak_location']*14/14.01565).round(4)\n",
    "    input_file['KMD_C2H4O'] = (input_file['peak_location'].round() - input_file['peak_location']*44/44.02621).round(4)\n",
    "    \n",
    "    # 4. Library Peak List\n",
    "    ## 4a. Check library file\n",
    "    lib_pks = len(library_file.index)\n",
    "    library_file.shape\n",
    "    # Convert all of the uppercase column labels to lowercase and replace the spaces with `_`.\n",
    "    library_file.columns = (\n",
    "        library_file.columns.str.lower()\n",
    "        .str.strip()\n",
    "        .str.replace(r'\\s+', '_', regex=True)\n",
    "    )\n",
    "    \n",
    "    ## 4b. Compute protonated/deprotonated mass of the library\n",
    "    list_suspects = library_file.assign(\n",
    "        mz_neg_H = library_file.loc[:, 'mass'] - PROTON,\n",
    "        nominal_mz_neg_H = round(library_file.loc[:, 'mass'] - PROTON).astype(int)\n",
    "    )\n",
    "    list_suspects.sort_values(by='mass', inplace=True)\n",
    "\n",
    "    # 5. Suspect List Matches\n",
    "    def search_suspect_list(input_file, list_suspects, ppm_error):\n",
    "        matches = pd.DataFrame()\n",
    "        \n",
    "        for index, row in input_file.iterrows():\n",
    "            \n",
    "            LOWER = row['peak_location'] - row['peak_location'] * ppm_error / 1E6\n",
    "            UPPER = row['peak_location'] + row['peak_location'] * ppm_error / 1E6\n",
    "                    \n",
    "            temp_matches = list_suspects[(list_suspects['mz_neg_H'] >= LOWER) & (list_suspects['mz_neg_H'] <= UPPER)]\n",
    "            \n",
    "            if len(temp_matches):\n",
    "                new_df = temp_matches.copy()\n",
    "                new_df.loc[:, 'exper_mz'] = row['peak_location']\n",
    "                new_df.loc[:, 'height'] = row['peak_height']\n",
    "                new_df.loc[:, 'abund'] = row['scaled_abundance']\n",
    "                new_df.loc[:, 'exp_KMD_CF2'] = row['KMD_CF2']\n",
    "                new_df.loc[:, 'exp_KMD_CH2'] = row['KMD_CH2']\n",
    "                new_df.loc[:, 'exp_KMD_C2H4O'] = row['KMD_C2H4O']\n",
    "                new_df.loc[:, 'S/N'] = row['s/n']\n",
    "                new_df.loc[:, 'ppm_error'] = 1E6 * (row['peak_location'] - temp_matches['mz_neg_H']) / temp_matches['mz_neg_H']          \n",
    "                matches = pd.concat([matches, new_df])\n",
    "                \n",
    "        return matches\n",
    "\n",
    "    results = search_suspect_list(input_file, list_suspects, ppm_error)\n",
    "    \n",
    "    # 6. Reformat and Print Results\n",
    "    results.rename(columns={'mass': 'lib_mass'}, inplace=True)\n",
    "    results.rename(columns={'mz_neg_H': 'library_neg_mz'}, inplace=True)\n",
    "    results.rename(columns={'c': 'C'}, inplace=True)\n",
    "    results.rename(columns={'h': 'H'}, inplace=True)\n",
    "    results.rename(columns={'br': 'Br'}, inplace=True)\n",
    "    results.rename(columns={'cl': 'Cl'}, inplace=True)\n",
    "    results.rename(columns={'f': 'F'}, inplace=True)\n",
    "    results.rename(columns={'i': 'I'}, inplace=True)\n",
    "    results.rename(columns={'n': 'N'}, inplace=True)\n",
    "    results.rename(columns={'o': 'O'}, inplace=True)\n",
    "    results.rename(columns={'p': 'P'}, inplace=True)\n",
    "    results.rename(columns={'s': 'S'}, inplace=True)\n",
    "    results.rename(columns={'dbe': 'DBE'}, inplace=True)\n",
    "    results.columns\n",
    "    results = results.loc[:, ['exper_mz', 'height', 'abund', 'lib_mass', 'ppm_error', 'S/N', 'formula', \n",
    "                            'C', 'H', 'Br', 'Cl', 'F', 'I', 'N', 'O', 'P', 'S', 'DBE']]\n",
    "    results = results.reset_index()\n",
    "    \n",
    "    # Remove those don't have 'H' (which means they can't be deprotonated)\n",
    "    # results.drop_duplicates(subset=['formula'],inplace=True)\n",
    "    results = results.loc[results['formula'].str.contains('H')]\n",
    "    \n",
    "    #Count the number of unique peaks have assigned\n",
    "    num_unique = results.loc[:, 'exper_mz'].nunique()\n",
    "\n",
    "    # Save the temporary results so far:\n",
    "    temp_file_name = os.path.splitext(os.path.basename(input_file_path))[0] + '_pre_processed.csv'\n",
    "    temp_file = os.path.join(input_folder, temp_file_name)\n",
    "    if os.path.exists(temp_file):\n",
    "        continue\n",
    "    results.to_csv(temp_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
