{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gg/9t2489gd6kv3l8nkxgtv9rrc0000gn/T/ipykernel_13552/966314758.py:101: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  CF2_series = CF2_series.groupby(['KMD_CF2', 'z*_CF2', 'H', 'Cl', 'Br', 'P', 'S'], group_keys=False).apply(to_category_E)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Load Files Here: (extension name needed)\n",
    "input_folder = 'test/Na_K_NH4_adducts/'\n",
    "output_folder = 'test/Na_K_NH4_adducts/'\n",
    "csv_files = glob.glob(os.path.join(input_folder, '*.csv'))\n",
    "# NIST = pd.read_csv('../PFAS_libraries/NIST_PFAS_Suspect_List_corrected.csv')\n",
    "NIST = pd.read_csv('../PFAS_libraries/NIST_PFAS_Suspect_List_pos.csv')\n",
    "    \n",
    "# Loop through each file in the folder\n",
    "for input_file_path in csv_files:\n",
    "    if input_file_path.endswith('_series.csv'):\n",
    "        continue\n",
    "    # Input File:\n",
    "    df = pd.read_csv(input_file_path)\n",
    "    df['ppm_error'] = df['ppm_error'].round(4)\n",
    "    df['exper_mz'] = df['exper_mz'].round(5) \n",
    "    df.insert(4, 'abund_norm', np.nan)\n",
    "    df['abund_norm'] = df['abund'] / df['abund'].max()\n",
    "\n",
    "    # Output File:\n",
    "    output_file_name_stem = os.path.splitext(os.path.basename(input_file_path))[0] \n",
    "    output_file_name = f\"{output_file_name_stem.replace('processed', 'series')}.xlsx\" \n",
    "    output_file = os.path.join(output_folder, output_file_name)\n",
    "    if os.path.exists(output_file):\n",
    "        continue\n",
    "         \n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    \n",
    "    \"\"\"1. Break the input file into 3 subframes: df1 = CF2 series only; df2 = CH2 series only; and df3 = C2H4O series only\"\"\"\n",
    "    dfs = []\n",
    "    start_index = 0\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        if i == len(df) - 1:\n",
    "            dfs.append(df.iloc[start_index:])\n",
    "            break\n",
    "        \n",
    "        diff = abs(df.iloc[i]['exper_mz'] - df.iloc[i-1]['exper_mz'])\n",
    "        \n",
    "        if diff >= 500:\n",
    "            dfs.append(df.iloc[start_index:i])\n",
    "            start_index = i\n",
    "    if len(dfs) == 3:\n",
    "        df1, df2, df3 = dfs\n",
    "    else:\n",
    "        print(f\"Skipping file {input_file_path} as it does not have the expected number of breakpoints.\")\n",
    "        continue\n",
    "        \n",
    "    df1, df2, df3 = dfs\n",
    "    \n",
    "    \"\"\"2. Data cleanup\"\"\"\n",
    "    df1 = df1.drop_duplicates()\n",
    "    df2 = df2.drop_duplicates()\n",
    "    df3 = df3.drop_duplicates()\n",
    "    \n",
    "    # Please do your own statistics and decide what criteria you should use for better credibility \n",
    "    df1 = df1[df1['I'] == 0]\n",
    "    df1 = df1[df1['F'] >= 5]  \n",
    "    \n",
    "    all_iso_values = df1[['13C_iso?', '81Br_iso?', '37Cl_iso?', '18O_iso?', '34S_iso?']].values.flatten()\n",
    "    df1['isotope?'] = df1['exper_mz'].apply(lambda x: 'Y' if x in all_iso_values else 'N')\n",
    "    \n",
    "    # Isotope peaks are removed so we may see some members are gone in a homologous series. Warning: 18-oxygen may have +/- 60%\n",
    "    # isotopic abundance error so this criteria may or may not remove too many        \n",
    "    df1 = df1[df1['isotope?'] == 'N']\n",
    "    df1.drop(columns=['isotope?'])\n",
    "    \n",
    "    # This function limits the delta C number to be 1 or 2 only, corresponding to CF2 or C2F4    \n",
    "    def carbon_number_checker(group):\n",
    "        sorted_C = sorted(group['C'].unique())  \n",
    "        differences = [sorted_C[i+1] - sorted_C[i] for i in range(len(sorted_C)-1)]  \n",
    "        return all(diff in {1, 2} for diff in differences) \n",
    "    CF2_series = df1.groupby(['KMD_CF2', 'z*_CF2', 'H', 'Cl', 'Br', 'P', 'S']).filter(carbon_number_checker)\n",
    "    \n",
    "    # You can also clean up data for other homologous series such as CH2 or C2H4O: \n",
    "    # CH2_series = df2.sort_values(['KMD_CH2', 'z*_CH2', 'F', 'Cl', 'Br', 'P', 'S'])\n",
    "    # C2H4O_series = df3.sort_values(['KMD_C2H4O', 'z*_C2H4O', 'F', 'Cl', 'Br', 'P', 'S'])\n",
    "\n",
    "    \n",
    "    \"\"\"3. Categorization (confidence levels)\"\"\" \n",
    "    # Cat A - known PFAS with all members having unique formula in a homologous series (>=3 members); \n",
    "    # Cat B - unknown PFAS with all members having unique formula in a homologous series (>=3 members); \n",
    "    # Cat C - unknown PFAS with at least 1 member having unique formula in a homologous series (>=3 members); \n",
    "    # Cat D - unknown PFAS with all members having multiple assignments but at least 1 member having correct isotopologues;\n",
    "    # Cat E - no isotope fine structure for any member in a CF2 homologous series - they may be false positive!\n",
    "    # Note, we will define the length cutoff for each homologous series in Step 4\n",
    "    \n",
    "    \n",
    "    \"\"\"3-1. Category E (no isotope fine structure for any member in a CF2 homologous series):\"\"\"\n",
    "    CF2_series.insert(13, 'category', np.nan)\n",
    "    columns_to_check = [\"13C_match?\", \"81Br_match?\", \"37Cl_match?\", \"18O_match?\", \"34S_match?\"]\n",
    "\n",
    "    def to_category_E(group):\n",
    "        if not (group[columns_to_check] == \"Y\").any().any():  \n",
    "            group[\"category\"] = \"E\"  \n",
    "        return group\n",
    "    CF2_series = CF2_series.groupby(['KMD_CF2', 'z*_CF2', 'H', 'Cl', 'Br', 'P', 'S'], group_keys=False).apply(to_category_E)\n",
    "    \n",
    "    \n",
    "    \"\"\"3-2. Uniform the KMD_CF2 values for different assignments of a peak:\"\"\"\n",
    "    CF2_series.insert(7, 'KMD_CF2_new', np.nan)\n",
    "    CF2_series['KMD_CF2_new'] = CF2_series['KMD_CF2']\n",
    "            \n",
    "              \n",
    "    \"\"\"3-3. Correct the KMD values (this is because different assignments of a peak may have slightly different KMD values.\"\"\"\n",
    "    ## We need to unify all KMD values of different assignments):  \n",
    "    CF2_series['KMD_CF2_new'] = CF2_series.groupby('exper_mz')['KMD_CF2'].transform('max')\n",
    "    CF2_series['KMD_CF2_new'] = CF2_series.groupby(['KMD_CF2', 'z*_CF2'])['KMD_CF2_new'].transform('max')\n",
    "    CF2_series['KMD_CF2_new'] = CF2_series.groupby('exper_mz')['KMD_CF2_new'].transform('max')\n",
    "    CF2_series['KMD_CF2_new'] = CF2_series.groupby(['KMD_CF2', 'z*_CF2'])['KMD_CF2_new'].transform('max')\n",
    "    \n",
    "            \n",
    "    \"\"\"3-4. Categories B, C, D:\"\"\"\n",
    "    ## Add helper columns 'group_mz_count_max' and 'group_mz_count_min'\n",
    "    CF2_series.insert(7, 'mz_count_max', np.nan)\n",
    "    CF2_series.insert(7, 'mz_count_min', np.nan)\n",
    "    CF2_series['mz_count_max'] = CF2_series.groupby(['KMD_CF2_new', 'z*_CF2'])['exper_mz'].transform(lambda x: x.value_counts().max())\n",
    "    CF2_series['mz_count_min'] = CF2_series.groupby(['KMD_CF2_new', 'z*_CF2'])['exper_mz'].transform(lambda x: x.value_counts().min())\n",
    "    \n",
    "    def count_ones(x):\n",
    "        counts = x.value_counts()\n",
    "        return sum(counts == 1)\n",
    "    CF2_series.insert(14, 'mz_count_ones', np.nan)\n",
    "    CF2_series['mz_count_ones'] = CF2_series.groupby(['KMD_CF2_new', 'z*_CF2'])['exper_mz'].transform(count_ones)\n",
    "\n",
    "    # The 'conditions' below means: \n",
    "    # (1) In a CF2 series, if the max occurence of each m/z (max number of assignments a peak could have) and min occurence of each m/z\n",
    "    # (min number of assignments a peak could have) are both > 1, this means all members have more than 1 assignment and therefore are Cat D/E. \n",
    "    # (2) If the max occurence of each m/z > 1 but min occurence of each m/z = 1, this means at least 1 members have unique assignments whereas\n",
    "    # other members have more than 1 assignment. In this case, they are all in Cat C.\n",
    "    # (3) If the max occurence of each m/z and min occurence of each m/z = 1, or only 1 member has unique assignment,\n",
    "    # this means all members have only 1 assignment and therefore should be Cat B.\n",
    "\n",
    "    conditions = [\n",
    "        (CF2_series['mz_count_max'] > 1) & (CF2_series['mz_count_min'] > 1), \n",
    "        (CF2_series['mz_count_max'] > 1) & (CF2_series['mz_count_min'] == 1) & (CF2_series['mz_count_ones'] >= 1), \n",
    "        (CF2_series['mz_count_max'] == 1), \n",
    "    ]\n",
    "    which_category = ['D', 'C', 'B']\n",
    "    no_category_E = CF2_series['category'] != 'E'\n",
    "    new_category = np.select(conditions, which_category, default=CF2_series['category'])\n",
    "    CF2_series.loc[no_category_E, 'category'] = new_category[no_category_E]\n",
    "    \n",
    "    \"\"\"3-5. Category A:\"\"\"\n",
    "    found_in_NIST = CF2_series['formula'].isin(NIST['formula'])\n",
    "    CF2_series.loc[found_in_NIST, 'category'] = 'A'\n",
    "    CF2_series['category'] = CF2_series.groupby(['KMD_CF2', 'z*_CF2', 'H', 'P', 'S'])['category'].transform(lambda x: 'A' if 'A' in x.values else x)    \n",
    "    # Here groupby(['KMD_CF2', ... should be used instead of groupby(['KMD_CF2_new', ... !\n",
    "    \n",
    "    def alt_assgn_of_cat_A(x):\n",
    "        if 'A' in x.values or 'A' in x.values :\n",
    "            return x.replace({'B':'A alt assgn', 'C':'A alt assgn', 'D':'A alt assgn', 'E':'A alt assgn'})\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    CF2_series['category'] = CF2_series.groupby(['exper_mz'])['category'].transform(alt_assgn_of_cat_A)\n",
    "    CF2_series['category'] = CF2_series.groupby(['KMD_CF2', 'z*_CF2', 'H', 'P', 'S'])['category'].transform(alt_assgn_of_cat_A)\n",
    "    CF2_series = CF2_series[~CF2_series['category'].isin(['A alt assgn'])]\n",
    "    \n",
    "    \"\"\"4. Keep series with 4 or above members for UNKNOWN PFAS (Cat B, C, D, and E) only\"\"\"\n",
    "    \n",
    "    # First, Cat B, C, and D:      \n",
    "    CF2_series_BCD = CF2_series[CF2_series['category'].isin(['B', 'C', 'D'])]\n",
    "    CF2_series_BCD = CF2_series_BCD.groupby(['KMD_CF2_new', 'z*_CF2', 'H', 'Cl', 'Br', 'P', 'S']).filter(lambda x: len(x) >= 4)\n",
    "    \n",
    "    # Then we need to update the categorization of previous Cat C and D homologous series which have 3 members but have been removed  \n",
    "    # as we now require 4 members. For example, for m/z 500-550-600-650, 500 has 1 assignment while the others have 2. Then the second set of\n",
    "    # assignment 550-600-650 has been removed because we now only allow series with at least 4 members. Therefore, only the first set of assignment\n",
    "    # 500-550-600-650 will be remained. \n",
    "    # This set, which was previously a Cat C or D series, now needs to be updated to Cat B. \n",
    "    # Nothing to do with Cat E because they don't have isotope fine structures!\n",
    "\n",
    "    CF2_series_BCD['mz_count_max'] = np.nan\n",
    "    CF2_series_BCD['mz_count_min'] = np.nan\n",
    "    CF2_series_BCD['mz_count_max'] = CF2_series_BCD.groupby(['KMD_CF2_new', 'z*_CF2'])['exper_mz'].transform(lambda x: x.value_counts().max())\n",
    "    CF2_series_BCD['mz_count_min'] = CF2_series_BCD.groupby(['KMD_CF2_new', 'z*_CF2'])['exper_mz'].transform(lambda x: x.value_counts().min())\n",
    "    CF2_series_BCD['mz_count_ones'] = CF2_series_BCD.groupby(['KMD_CF2_new', 'z*_CF2'])['exper_mz'].transform(count_ones)\n",
    "    conditions = [\n",
    "        ((CF2_series_BCD['mz_count_max'] > 1) & (CF2_series_BCD['mz_count_min'] > 1)), \n",
    "        (CF2_series_BCD['mz_count_max'] > 1) & (CF2_series_BCD['mz_count_min'] == 1) & (CF2_series_BCD['mz_count_ones'] >= 1), \n",
    "        (CF2_series_BCD['mz_count_max'] == 1), \n",
    "    ]\n",
    "    CF2_series_BCD['category'] = np.select(conditions, which_category, default='nan')\n",
    "    \n",
    "    # Now Cat E:      \n",
    "    CF2_series_E = CF2_series[CF2_series['category'].isin(['E'])]\n",
    "    CF2_series_E = CF2_series_E.groupby(['KMD_CF2_new', 'z*_CF2', 'H', 'Cl', 'Br', 'P', 'S']).filter(lambda x: len(x) >= 4)\n",
    "    \n",
    "    # Finally piece them together:\n",
    "    CF2_series = pd.concat([CF2_series[~CF2_series['category'].isin(['B', 'C', 'D', 'E'])], CF2_series_BCD, CF2_series_E])\n",
    "    number_of_A = CF2_series[CF2_series['category'] == 'A']['KMD_CF2_new'].nunique()\n",
    "    number_of_B = CF2_series[CF2_series['category'] == 'B']['KMD_CF2_new'].nunique()\n",
    "    number_of_C = CF2_series[CF2_series['category'] == 'C']['KMD_CF2_new'].nunique()\n",
    "    number_of_D = CF2_series[CF2_series['category'] == 'D']['KMD_CF2_new'].nunique()\n",
    "    number_of_E = CF2_series[CF2_series['category'] == 'E']['KMD_CF2_new'].nunique()\n",
    "    \n",
    "    \"\"\"5. Stats, and output of results\"\"\"  \n",
    "    # Drop helper lines first:\n",
    "    CF2_series = CF2_series.drop(columns=['mz_count_max', 'mz_count_min', 'mz_count_ones', 'KMD_CH2', 'z*_CH2', 'KMD_C2H4O', 'z*_C2H4O', '#raw_pks'])\n",
    "    \n",
    "    CF2_series = CF2_series.iloc[:, :-5]\n",
    "    CF2_series = CF2_series.sort_values(['KMD_CF2_new', 'z*_CF2'])\n",
    "    CF2_groups_num = CF2_series.groupby(['KMD_CF2_new', 'z*_CF2']).ngroup().max() + 1\n",
    "    CF2_series = CF2_series.drop(columns=['KMD_CF2_new'])\n",
    "    # CF2_series.insert(10, '(CF2)n', np.where(CF2_series['DBE'] >= 0, round((CF2_series['C']-1)/2), round((CF2_series['C']-6)/2)))\n",
    "    \n",
    "    CF2_series = CF2_series.reset_index(drop=True)\n",
    "    CF2_series.loc[0, 'stats'] = 'CF2 series:'\n",
    "    CF2_series.loc[1, 'stats'] = 'Cat A:'\n",
    "    CF2_series.loc[2, 'stats'] = 'Cat B:'\n",
    "    CF2_series.loc[3, 'stats'] = 'Cat C:'\n",
    "    CF2_series.loc[4, 'stats'] = 'Cat D:'\n",
    "    CF2_series.loc[5, 'stats'] = 'Cat E:'\n",
    "    \n",
    "    CF2_series.loc[0, '#_pks'] = int(CF2_series.loc[:, 'exper_mz'].nunique())\n",
    "    if CF2_series.loc[0, '#_pks'] == 0:\n",
    "        CF2_series.loc[0, '#_assgns'] = 0\n",
    "    else:\n",
    "        CF2_series.loc[0, '#_assgns'] = int(len(CF2_series.index))\n",
    "    try:\n",
    "        CF2_series.loc[0, '#_series'] = int(CF2_groups_num)\n",
    "        CF2_series.loc[1, '#_series'] = int(number_of_A)\n",
    "        CF2_series.loc[2, '#_series'] = int(number_of_B)\n",
    "        CF2_series.loc[3, '#_series'] = int(number_of_C)\n",
    "        CF2_series.loc[4, '#_series'] = int(number_of_D)\n",
    "        CF2_series.loc[5, '#_series'] = int(number_of_E)\n",
    "    except ValueError as e:\n",
    "        if 'cannot convert float NaN to integer' in str(e):\n",
    "            CF2_series.loc[0, '#_series'] = 0\n",
    "            \n",
    "        \n",
    "    \"\"\"6. Write the results into an excel file\"\"\"\n",
    "    with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "        CF2_series.to_excel(writer, sheet_name='CF2_series', index=False)\n",
    "        # CH2_series.to_excel(writer, sheet_name='CH2_series', index=False)\n",
    "        # C2H4O_series.to_excel(writer, sheet_name='C2H4O_series', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
